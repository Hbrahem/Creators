{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "935ba634",
   "metadata": {},
   "source": [
    "# Analyse sentiment :regression lin√©aire"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f46d95a",
   "metadata": {},
   "source": [
    "import dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4f2499d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk import pos_tag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf16cea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/cardinal/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44478ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f76ef5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data from csv file to pandas dataframe\n",
    "df = pd.read_excel(r\"./DisneylandReviews.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "884328c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review_ID</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Year_Month</th>\n",
       "      <th>Reviewer_Location</th>\n",
       "      <th>Review_Text</th>\n",
       "      <th>Branch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>670772142</td>\n",
       "      <td>4</td>\n",
       "      <td>2019-4</td>\n",
       "      <td>Australia</td>\n",
       "      <td>If you've ever been to Disneyland anywhere you...</td>\n",
       "      <td>Disneyland_HongKong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>670682799</td>\n",
       "      <td>4</td>\n",
       "      <td>2019-5</td>\n",
       "      <td>Philippines</td>\n",
       "      <td>Its been a while since d last time we visit HK...</td>\n",
       "      <td>Disneyland_HongKong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>670623270</td>\n",
       "      <td>4</td>\n",
       "      <td>2019-4</td>\n",
       "      <td>United Arab Emirates</td>\n",
       "      <td>Thanks God it wasn   t too hot or too humid wh...</td>\n",
       "      <td>Disneyland_HongKong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>670607911</td>\n",
       "      <td>4</td>\n",
       "      <td>2019-4</td>\n",
       "      <td>Australia</td>\n",
       "      <td>HK Disneyland is a great compact park. Unfortu...</td>\n",
       "      <td>Disneyland_HongKong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>670607296</td>\n",
       "      <td>4</td>\n",
       "      <td>2019-4</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>the location is not in the city, took around 1...</td>\n",
       "      <td>Disneyland_HongKong</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Review_ID  Rating Year_Month     Reviewer_Location  \\\n",
       "0  670772142       4     2019-4             Australia   \n",
       "1  670682799       4     2019-5           Philippines   \n",
       "2  670623270       4     2019-4  United Arab Emirates   \n",
       "3  670607911       4     2019-4             Australia   \n",
       "4  670607296       4     2019-4        United Kingdom   \n",
       "\n",
       "                                         Review_Text               Branch  \n",
       "0  If you've ever been to Disneyland anywhere you...  Disneyland_HongKong  \n",
       "1  Its been a while since d last time we visit HK...  Disneyland_HongKong  \n",
       "2  Thanks God it wasn   t too hot or too humid wh...  Disneyland_HongKong  \n",
       "3  HK Disneyland is a great compact park. Unfortu...  Disneyland_HongKong  \n",
       "4  the location is not in the city, took around 1...  Disneyland_HongKong  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "798f771f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42656, 6)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check number rows and columns\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd76df3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Review_ID            0\n",
       "Rating               0\n",
       "Year_Month           0\n",
       "Reviewer_Location    0\n",
       "Review_Text          0\n",
       "Branch               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#counting number of missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3af4277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Rating\n",
       "5    23146\n",
       "4    10775\n",
       "3     5109\n",
       "2     2127\n",
       "1     1499\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking distribution of traget column\n",
    "df['Rating'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb46667",
   "metadata": {},
   "source": [
    "Stemming is the process of reducing a word to its Root word\n",
    "example:actor,actress,acting =act\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "308f15f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "port_stem=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33b15995",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(content):\n",
    "    \n",
    "    stemmed_content = re.sub('[^a-zA-Z]',' ',content)\n",
    "    stemmed_content = stemmed_content.lower()\n",
    "    stemmed_content = stemmed_content.split()\n",
    "    stemmed_content = [port_stem.stem(word) for word in stemmed_content if not word in stopwords.words('english')]\n",
    "    stemmed_content = ' '.join(stemmed_content)\n",
    "    return stemmed_content\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ef7a91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['stemmed'] = df['Review_Text'].apply(stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "939a1dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        ever disneyland anywher find disneyland hong k...\n",
      "1        sinc last time visit hk disneyland yet time st...\n",
      "2        thank god hot humid visit park otherwis would ...\n",
      "3        hk disneyland great compact park unfortun quit...\n",
      "4        locat citi took around hour kowlon kid like di...\n",
      "                               ...                        \n",
      "42651    went disneyland pari juli thought brilliant vi...\n",
      "42652    adult child visit disneyland pari begin feb ab...\n",
      "42653    eleven year old daughter went visit son london...\n",
      "42654    hotel part disneyland pari complex wonder plac...\n",
      "42655    went disneypari resort small child minut enter...\n",
      "Name: stemmed, Length: 42656, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print (df['stemmed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8caa007",
   "metadata": {},
   "outputs": [],
   "source": [
    "x= df['stemmed'].values\n",
    "y = df['Rating'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c3b68ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ever disneyland anywher find disneyland hong kong similar layout walk main street familiar feel one ride small world absolut fabul worth day visit fairli hot rel busi queue move fairli well'\n",
      " 'sinc last time visit hk disneyland yet time stay tomorrowland aka marvel land iron man experi n newli open ant man n wasp ironman great featur n excit especi whole sceneri hk hk central area kowloon antman chang previou buzz lightyear less expect someth howev boy like space mountain turn star war great cast member staff felt bit minu point dun feel like disney brand seem local like ocean park even worst got smile face wanna u enter n attract n leav hello suppos happiest place earth brand realli dont feel bakeri main street attract delicaci n disney theme sweet good point last also starbuck insid theme park'\n",
      " 'thank god hot humid visit park otherwis would big issu lot shade arriv around left pm unfortun last even parad hour much plenti everyon find someth interest enjoy extrem busi longest time queue certain attract minut realli bad although amaz time felt bit underwhelm choic ride attract park quit small realli expect someth grand even main castl close way quit small food option good coffe shop includ starbuck plenti gift shop issu toilet everywher togeth great day realli enjoy'\n",
      " ...\n",
      " 'eleven year old daughter went visit son london decid go disneyland pari stay two night three day hotel du moulon great valu money close disneyland highlight stay walt disney studio unfortun third day rush see everyth would suggest three night four day see went may would recommend line ride long weather also good colleen smit'\n",
      " 'hotel part disneyland pari complex wonder place famili sinc kid need room crew area consist nice bath separ toilet area shower tub larg room tv doubl bed plu smaller room bunk bed nd tv rate reason includ unlimit entri park also includ wonder breakfast servic fantast drawback throughout europ size bath cloth much larg larger hand towel carri wash cloth'\n",
      " 'went disneypari resort small child minut enter transport disneyland pari treat like special guest compnay realli know treat guest start finsh arriv disneyland resort hotel world like storybook come aliv front eye hotel santa fa base around mexico villiag room theme mexico style larg bed larg room one disadvantag swim pool would great french climat went june degre everyday hotel playground theme facil nice walk disneyland park boardwalk take around min foot revist disneyland pari resort would like stay hotel friendli help chang beach resort magic fun ad extra disney magic never forgotten michel pill']\n"
     ]
    }
   ],
   "source": [
    "print (x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "299bea90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 4 4 ... 5 4 4]\n"
     ]
    }
   ],
   "source": [
    "print (y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3075cf",
   "metadata": {},
   "source": [
    "Splitting the data to training data and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a5009fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, stratify=y,random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ec22785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42656,) (34124,) (8532,)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape, x_train.shape,x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb985261",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert textual data to numerical data\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "x_train =vectorizer.fit_transform(x_train)\n",
    "x_test = vectorizer.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b9608bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 8654)\t0.10678431495613067\n",
      "  (0, 9080)\t0.07893801542608998\n",
      "  (0, 7289)\t0.16447065025722712\n",
      "  (0, 24070)\t0.10017739035800663\n",
      "  (0, 1675)\t0.1537728441996045\n",
      "  (0, 23234)\t0.09464469766908681\n",
      "  (0, 20133)\t0.11468514638259764\n",
      "  (0, 21396)\t0.13816292856979437\n",
      "  (0, 2067)\t0.1225558665142813\n",
      "  (0, 23453)\t0.1688111293011756\n",
      "  (0, 12136)\t0.2041824747872766\n",
      "  (0, 12288)\t0.10546528655124612\n",
      "  (0, 1566)\t0.16491856795819518\n",
      "  (0, 18380)\t0.18434574113086483\n",
      "  (0, 11570)\t0.19083571526514292\n",
      "  (0, 16676)\t0.25085283190976154\n",
      "  (0, 23879)\t0.14402698770453787\n",
      "  (0, 13516)\t0.20729562394368023\n",
      "  (0, 21626)\t0.14001895535041797\n",
      "  (0, 23680)\t0.1858361794703714\n",
      "  (0, 13907)\t0.38643783061886056\n",
      "  (0, 9280)\t0.35866905920993686\n",
      "  (0, 21044)\t0.18972600649476198\n",
      "  (0, 5135)\t0.14187644493065305\n",
      "  (0, 21350)\t0.12106539851868067\n",
      "  :\t:\n",
      "  (34123, 7163)\t0.10448806963802935\n",
      "  (34123, 12554)\t0.10733924881635372\n",
      "  (34123, 8511)\t0.18522427634819372\n",
      "  (34123, 23895)\t0.11866496283276441\n",
      "  (34123, 287)\t0.13708381142191947\n",
      "  (34123, 11266)\t0.2654962789266982\n",
      "  (34123, 9141)\t0.3044500813584063\n",
      "  (34123, 11689)\t0.15727765484071363\n",
      "  (34123, 10166)\t0.15731914344397238\n",
      "  (34123, 21181)\t0.12084809582783093\n",
      "  (34123, 16820)\t0.11171453117051384\n",
      "  (34123, 12105)\t0.2044711853644036\n",
      "  (34123, 14750)\t0.1756322209641167\n",
      "  (34123, 13764)\t0.10558956076109312\n",
      "  (34123, 12387)\t0.09695957292432353\n",
      "  (34123, 12090)\t0.19534341852239814\n",
      "  (34123, 2913)\t0.17870269287560053\n",
      "  (34123, 17733)\t0.12856109445509167\n",
      "  (34123, 11793)\t0.14728591173879402\n",
      "  (34123, 5839)\t0.2951179098802855\n",
      "  (34123, 15238)\t0.259347356895457\n",
      "  (34123, 6863)\t0.10344323970676388\n",
      "  (34123, 20133)\t0.11686913891538184\n",
      "  (34123, 12288)\t0.10747370181303993\n",
      "  (34123, 11570)\t0.09723493591286236\n"
     ]
    }
   ],
   "source": [
    "print (x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b6e1570d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 21396)\t0.09563567886271504\n",
      "  (0, 14669)\t0.2809763002523209\n",
      "  (0, 14017)\t0.28510830317605446\n",
      "  (0, 13067)\t0.250360762291643\n",
      "  (0, 12539)\t0.19675305506377017\n",
      "  (0, 11861)\t0.192440113197033\n",
      "  (0, 9080)\t0.10928098834465554\n",
      "  (0, 8928)\t0.11516684353548579\n",
      "  (0, 8340)\t0.3354691984812789\n",
      "  (0, 8271)\t0.13180505667661171\n",
      "  (0, 7984)\t0.2832996240360636\n",
      "  (0, 7255)\t0.3226393965865864\n",
      "  (0, 7181)\t0.21760390760074638\n",
      "  (0, 5652)\t0.2888035679322954\n",
      "  (0, 3768)\t0.2770425273577704\n",
      "  (0, 3164)\t0.25508735967711427\n",
      "  (0, 2067)\t0.16966509922784015\n",
      "  (0, 80)\t0.22661385217324292\n",
      "  (1, 23895)\t0.21641229116834954\n",
      "  (1, 21238)\t0.2577733770305555\n",
      "  (1, 19267)\t0.2247314122029599\n",
      "  (1, 19125)\t0.3584957575574484\n",
      "  (1, 17733)\t0.1172300582327197\n",
      "  (1, 16820)\t0.20373661332099277\n",
      "  (1, 16778)\t0.33877934403259746\n",
      "  :\t:\n",
      "  (8530, 10108)\t0.13144968685431266\n",
      "  (8530, 9319)\t0.08001658291979714\n",
      "  (8530, 9141)\t0.08791276659373806\n",
      "  (8530, 5135)\t0.06262245373517497\n",
      "  (8530, 3506)\t0.19842230534978028\n",
      "  (8530, 2675)\t0.26574756493620233\n",
      "  (8530, 1446)\t0.23208984415734318\n",
      "  (8530, 666)\t0.13592428387058839\n",
      "  (8530, 655)\t0.09701192678340746\n",
      "  (8531, 22491)\t0.33529256589425155\n",
      "  (8531, 19871)\t0.46455793451209637\n",
      "  (8531, 16498)\t0.2619256316874879\n",
      "  (8531, 14752)\t0.2220991741941675\n",
      "  (8531, 14082)\t0.1342571281169798\n",
      "  (8531, 13764)\t0.09856009875698078\n",
      "  (8531, 12713)\t0.10761437547910808\n",
      "  (8531, 12310)\t0.1271288433455967\n",
      "  (8531, 12066)\t0.21306908883617037\n",
      "  (8531, 11086)\t0.2779767859268635\n",
      "  (8531, 11010)\t0.2537412031748172\n",
      "  (8531, 9319)\t0.08621901163962709\n",
      "  (8531, 7403)\t0.1045445036813486\n",
      "  (8531, 5449)\t0.2379712822063576\n",
      "  (8531, 4072)\t0.4718218922127112\n",
      "  (8531, 695)\t0.11698908591576702\n"
     ]
    }
   ],
   "source": [
    "print(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c35fbc9",
   "metadata": {},
   "source": [
    "training the machine learning model \n",
    "\n",
    "logistic regresion:classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8ec9be6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model =LogisticRegression(max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "75df7d29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=1000)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a49453",
   "metadata": {},
   "source": [
    "Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8c10d1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy score on the training data\n",
    "x_train_prediction = model.predict(x_train)\n",
    "training_data_accuracy = accuracy_score(y_train, x_train_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b9cc979a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy prdiction: 0.7196108310866253\n"
     ]
    }
   ],
   "source": [
    "print('accuracy prdiction:',training_data_accuracy )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "87649578",
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy score on the test data\n",
    "x_test_prediction = model.predict(x_test)\n",
    "test_data_accuracy = accuracy_score(y_test, x_test_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1330e030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy prdiction: 0.6274027191748711\n"
     ]
    }
   ],
   "source": [
    "print('accuracy prdiction:',test_data_accuracy )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c0d874",
   "metadata": {},
   "source": [
    "\n",
    "save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aff69a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2a3a7fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename ='sentiment.sav'\n",
    "pickle.dump(model, open(filename,'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd981722",
   "metadata": {},
   "source": [
    "using the saved model for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ec84e221",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading saved model\n",
    "loa_model = pickle.load(open('sendedtiment.sav','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8665723b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "lower not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m x_new \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi love this movie\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthis movie is great\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhat a fantastic movie\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      2\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer()\n\u001b[0;32m----> 3\u001b[0m x_train \u001b[38;5;241m=\u001b[39m\u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m x_test \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mtransform(x_test)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(y_test[\u001b[38;5;241m9\u001b[39m])\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:2139\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   2132\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[1;32m   2133\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[1;32m   2134\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[1;32m   2135\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[1;32m   2136\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[1;32m   2137\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[1;32m   2138\u001b[0m )\n\u001b[0;32m-> 2139\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2140\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[1;32m   2141\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[1;32m   2142\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:1389\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1381\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1382\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1383\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1384\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1385\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1386\u001b[0m             )\n\u001b[1;32m   1387\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1389\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[1;32m   1392\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:1276\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1274\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[1;32m   1275\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m-> 1276\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1277\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1278\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:110\u001b[0m, in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m preprocessor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 110\u001b[0m         doc \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    112\u001b[0m         doc \u001b[38;5;241m=\u001b[39m tokenizer(doc)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:68\u001b[0m, in \u001b[0;36m_preprocess\u001b[0;34m(doc, accent_function, lower)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Chain together an optional series of text preprocessing steps to\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03mapply to a document.\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    preprocessed string\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lower:\n\u001b[0;32m---> 68\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m()\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accent_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m     doc \u001b[38;5;241m=\u001b[39m accent_function(doc)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/scipy/sparse/_base.py:771\u001b[0m, in \u001b[0;36mspmatrix.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgetnnz()\n\u001b[1;32m    770\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 771\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(attr \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m not found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: lower not found"
     ]
    }
   ],
   "source": [
    "x_new = \"i love this movie\"\n",
    "vectorizer = TfidfVectorizer()\n",
    "x_train =vectorizer.fit_transform(x_train)\n",
    "x_test = vectorizer.transform(x_test)\n",
    "print(y_test[9])\n",
    "\n",
    "prediction = model.predict(x_new)\n",
    "print(prediction)\n",
    "\n",
    "if(prediction[0]==2):\n",
    "    print('negative')\n",
    "else:\n",
    "    print('positive')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

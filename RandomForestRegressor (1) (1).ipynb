{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 183,
      "metadata": {
        "id": "Fo198x4bEq92"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 184,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "MKPrRKMsE_-j",
        "outputId": "9b398251-c273-4d63-be8e-9e605f2760a7"
      },
      "outputs": [],
      "source": [
        "# from google.colab import files\n",
        "# uploaded = files.upload()# Get the filename of the uploaded file\n",
        "# orders = list(uploaded.keys())[0]\n",
        "\n",
        "# Load the data from Excel file into a DataFrame\n",
        "data = pd.read_csv(\"orders.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {
        "id": "zPn6XFYpFAAq"
      },
      "outputs": [],
      "source": [
        "duplicates = data[data.duplicated('user_id')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "metadata": {
        "id": "Yz1EMrxzFADU"
      },
      "outputs": [],
      "source": [
        "#Ensure that each user ID corresponds to a unique user\n",
        "data = data.drop_duplicates('user_id')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "metadata": {
        "id": "oWAdzM_yGanl"
      },
      "outputs": [],
      "source": [
        "data['access_datetime'] = pd.to_datetime(data['access_date'] + ' ' + data['access_time'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "metadata": {
        "id": "VEvPMMV-Gap6"
      },
      "outputs": [],
      "source": [
        "# Derive additional time-related features\n",
        "data['hourly_attendance'] = data['access_datetime'].dt.hour\n",
        "data['daily_attendance'] = data['access_datetime'].dt.date"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "metadata": {
        "id": "t2hiBaU4Gar0"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Calculate total visits per day\n",
        "total_visits_per_day = data.groupby('daily_attendance').size()\n",
        "\n",
        "# Calculate total visitors per day\n",
        "total_visitors_per_day = data.groupby('daily_attendance')['user_id'].nunique()\n",
        "\n",
        "# Calculate peak visiting hours\n",
        "peak_visiting_hours = data.groupby('hourly_attendance').size().idxmax()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hzH6GhLtGavd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 190,
      "metadata": {
        "id": "Br-FRnIlG9eg"
      },
      "outputs": [],
      "source": [
        "start_time = time.time()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 191,
      "metadata": {
        "id": "CvYQbMooFAFs"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Aggregate hourly attendance data to obtain total visits per day\n",
        "total_visits_per_day = data.groupby('access_date')['hourly_attendance'].sum()\n",
        "\n",
        "# Merge aggregated data with the original DataFrame\n",
        "data = data.merge(total_visits_per_day, on='access_date', suffixes=('', '_total'))\n",
        "\n",
        "# Define features (X) and target variable (y)\n",
        "X = data[['access_date']]  # Features\n",
        "y = data['hourly_attendance_total']  # Target variable\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7EoqYe7QO47",
        "outputId": "8b25ba86-951f-45f1-f189-990c1bfd8897"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training time: 0.04449915885925293 seconds\n"
          ]
        }
      ],
      "source": [
        "# Record the end time\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate the time taken\n",
        "train_time = end_time - start_time\n",
        "print(\"Training time:\", train_time, \"seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "metadata": {
        "id": "gLQ-OM-GFAJM"
      },
      "outputs": [],
      "source": [
        "# Concatenate the training and testing data before applying one-hot encoding\n",
        "combined_data = pd.concat([X_train, X_test])\n",
        "combined_data_encoded = pd.get_dummies(combined_data)\n",
        "\n",
        "# Split the combined encoded data back into training and testing sets\n",
        "X_train_encoded = combined_data_encoded[:len(X_train)]\n",
        "X_test_encoded = combined_data_encoded[len(X_train):]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 194,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6sJkoqukG9h6",
        "outputId": "65422e83-a82a-4d16-c48e-c048be7ef759"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RMSE: 3.5834341071101057\n",
            "R^2 Score: 0.737938775510204\n",
            "   access_date_2023-04-01  access_date_2023-04-03  access_date_2023-04-04\n",
            "0                      32                       6                      37\n",
            "1                      53                      44                       7\n",
            "2                      95                      67                      45\n",
            "3                      49                      71                      19\n",
            "4                      46                      44                      93\n",
            "5                      92                      64                      10\n",
            "6                      19                      25                      22\n",
            "7                      65                      85                      87\n",
            "8                      93                     100                      16\n",
            "9                      76                      15                      75\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "The feature names should match those that were passed during fit.\nFeature names seen at fit time, yet now missing:\n- access_date_2023-06-08\n- access_date_2023-08-03\n- access_date_2023-08-14\n- access_date_2023-08-20\n- access_date_2023-11-04\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[194], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m d[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccess_date_2023-04-04\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m [randint(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m100\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m)]\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(d)\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m)\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:984\u001b[0m, in \u001b[0;36mForestRegressor.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    982\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    983\u001b[0m \u001b[38;5;66;03m# Check data\u001b[39;00m\n\u001b[0;32m--> 984\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_X_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    986\u001b[0m \u001b[38;5;66;03m# Assign chunk of trees to jobs\u001b[39;00m\n\u001b[1;32m    987\u001b[0m n_jobs, _, _ \u001b[38;5;241m=\u001b[39m _partition_estimators(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_estimators, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:599\u001b[0m, in \u001b[0;36mBaseForest._validate_X_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    596\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;124;03mValidate X whenever one tries to predict, apply, predict_proba.\"\"\"\u001b[39;00m\n\u001b[1;32m    598\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 599\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDTYPE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(X) \u001b[38;5;129;01mand\u001b[39;00m (X\u001b[38;5;241m.\u001b[39mindices\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mintc \u001b[38;5;129;01mor\u001b[39;00m X\u001b[38;5;241m.\u001b[39mindptr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mintc):\n\u001b[1;32m    601\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo support for np.int64 index based sparse matrices\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/base.py:580\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_data\u001b[39m(\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    511\u001b[0m     X\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params,\n\u001b[1;32m    517\u001b[0m ):\n\u001b[1;32m    518\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate input data and set or check the `n_features_in_` attribute.\u001b[39;00m\n\u001b[1;32m    519\u001b[0m \n\u001b[1;32m    520\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[38;5;124;03m        validated.\u001b[39;00m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 580\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_feature_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    582\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_tags()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires_y\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    583\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    584\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m estimator \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    585\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires y to be passed, but the target y is None.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    586\u001b[0m         )\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/base.py:507\u001b[0m, in \u001b[0;36mBaseEstimator._check_feature_names\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m missing_names \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m unexpected_names:\n\u001b[1;32m    503\u001b[0m     message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    504\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature names must be in the same order as they were in fit.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    505\u001b[0m     )\n\u001b[0;32m--> 507\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n",
            "\u001b[0;31mValueError\u001b[0m: The feature names should match those that were passed during fit.\nFeature names seen at fit time, yet now missing:\n- access_date_2023-06-08\n- access_date_2023-08-03\n- access_date_2023-08-14\n- access_date_2023-08-20\n- access_date_2023-11-04\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Concatenate the training and testing data before applying one-hot encoding\n",
        "combined_data = pd.concat([X_train, X_test])\n",
        "combined_data_encoded = pd.get_dummies(combined_data)\n",
        "\n",
        "# Split the combined encoded data back into training and testing sets\n",
        "X_train_encoded = combined_data_encoded[:len(X_train)]\n",
        "X_test_encoded = combined_data_encoded[len(X_train):]\n",
        "\n",
        "# Initialize and train the Linear Regression model\n",
        "model = RandomForestRegressor(random_state=42)\n",
        "model.fit(X_train_encoded, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = model.predict(X_test_encoded)\n",
        "\n",
        "# Evaluate the model's performance\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = mse ** 0.5\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"RMSE:\", rmse)\n",
        "print(\"R^2 Score:\", r2)\n",
        "from random import randint\n",
        "d = pd.DataFrame()\n",
        "d[\"access_date_2023-04-01\"] = [randint(0,100) for i in range(10)]\n",
        "d[\"access_date_2023-04-03\"] = [randint(0,100) for i in range(10)]\n",
        "d[\"access_date_2023-04-04\"] = [randint(0,100) for i in range(10)]\n",
        "print(d)\n",
        "print(model.predict(d))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# save the trained model\n",
        "import pickle\n",
        "filename ='RFG.sav'\n",
        "pickle.dump(model, open(filename,'wb'))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
